{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9024322a-060d-4990-8152-7a558880c370",
   "metadata": {},
   "source": [
    "# SST-2 Binary Text Classification with BERT Model (ref: [Transformers](https://huggingface.co/docs/transformers/training))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c03497b-45de-45f3-a575-634399a7f7ac",
   "metadata": {},
   "source": [
    "## Common Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0be39e9-cacc-44da-9319-a055d32b1110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Set some arguments for training\")\n",
    "parser.add_argument(\"--gpu_num\", type=int, help=\"gpu num you want to use\", default=0)\n",
    "parser.add_argument(\"--num_train_data\", type=int, help=\"the number of the training data\", default=32)\n",
    "parser.add_argument(\"--num_seed\", type=int, help=\"the number of the seeds\", default=10)\n",
    "parser.add_argument(\"--num_epochs\", type=int, help=\"the number of the epochs\", default=300)\n",
    "parser.add_argument(\"--backt\", action=\"store_true\", help=\"augment training data by backtranslation\")\n",
    "parser.add_argument(\"--eda\", action=\"store_true\", help=\"augment training data by EDA\")\n",
    "parser.add_argument(\"--masked_lm\", action=\"store_true\", help=\"augment training data by masked language model\")\n",
    "parser.add_argument(\"--afinn\", action=\"store_true\", help=\"augment training data by AFIIN using unlabeled data\")\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6affcd-ab60-42ec-99a1-e0fec74c51bd",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8e333a-2618-4ec7-a68c-b34bfb79fbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "def load_train_test_dataset(seed, num_train_data):\n",
    "    dataset = load_dataset(\"sst2\")\n",
    "\n",
    "    # idx, sentence, label\n",
    "    pre_train_dataset = dataset[\"train\"].shuffle(seed=seed)\n",
    "    train_dataset = pre_train_dataset.select(range(num_train_data))\n",
    "    train_rest_dataset = pre_train_dataset.select(range(num_train_data, len(pre_train_dataset)))\n",
    "    test_dataset = dataset[\"validation\"]\n",
    "    \n",
    "    return (train_dataset, train_rest_dataset, test_dataset)\n",
    "\n",
    "# train_dataset, train_rest_dataset, test_dataset = load_train_test_dataset(seed=0, num_train_data=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2211782-a9bb-4208-a160-e9627cfd6c4d",
   "metadata": {},
   "source": [
    "## Data Augmentation by Backtranslation (ref: [En to Fr](https://huggingface.co/Helsinki-NLP/opus-mt-en-fr?text=My+name+is+Sarah+and+I+live+in+London), [Fr to En](https://huggingface.co/Helsinki-NLP/opus-mt-fr-en?text=Mon+nom+est+Wolfgang+et+je+vis+%C3%A0+Berlin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c132094-84e9-4324-ae34-fe354f132007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "if args.backt:\n",
    "    en_to_others = [pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-fr\"), pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-de\")]\n",
    "    others_to_en = [pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\"), pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-de-en\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd79da2-755b-4471-987e-1650d6801edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_by_backt(train_dataset, en_to_others, others_to_en):\n",
    "    \n",
    "    sentences = [train_data[\"sentence\"] for train_data in train_dataset]\n",
    "    labels = [train_data[\"label\"] for train_data in train_dataset]\n",
    "    sentences_len = len(sentences)\n",
    "    \n",
    "    aug_by_backt_train_dataset = train_dataset\n",
    "    for translator_idx in range(len(en_to_others)):\n",
    "        tmp_sentence = [tmp_data['translation_text'] for tmp_data in en_to_others[translator_idx](sentences)]\n",
    "        aug_sentence = [tmp_data['translation_text'] for tmp_data in others_to_en[translator_idx](tmp_sentence)]\n",
    "        \n",
    "        for sen_idx in range(sentences_len):\n",
    "            aug_data = {'sentence': aug_sentence[sen_idx], 'label': labels[sen_idx]}\n",
    "            aug_by_backt_train_dataset = aug_by_backt_train_dataset.add_item(aug_data)\n",
    "            \n",
    "    return aug_by_backt_train_dataset\n",
    "\n",
    "# train_dataset = aug_by_backt(train_dataset, en_to_others, others_to_en)\n",
    "# aug_train_dataset = aug_by_backt(train_dataset, en_to_others, others_to_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87983ce2-af58-455b-af93-3e98dc7b0e35",
   "metadata": {},
   "source": [
    "## Data Augmentation by EDA (ref: [EDA](https://github.com/jasonwei20/eda_nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11cc973-08cd-4cd8-8eac-fa9669e12b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy data augmentation techniques for text classification\n",
    "# Jason Wei and Kai Zou\n",
    "\n",
    "from eda import *\n",
    "\n",
    "# For the first time to load wordnet\n",
    "'''\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "'''\n",
    "\n",
    "# Generate more data with EDA\n",
    "def aug_by_eda(train_dataset, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, alpha_rd=0.1, num_aug=3):\n",
    "    sentences = [train_data[\"sentence\"] for train_data in train_dataset]\n",
    "    labels = [train_data[\"label\"] for train_data in train_dataset]\n",
    "    \n",
    "    aug_by_eda_train_dataset = train_dataset\n",
    "    aug_sentences = [eda(sentence) for sentence in sentences]\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        for aug_sentence in aug_sentences[i]:\n",
    "            aug_data = {'sentence': aug_sentence, 'label': labels[i]}\n",
    "            aug_by_eda_train_dataset = aug_by_eda_train_dataset.add_item(aug_data)\n",
    "            \n",
    "    return aug_by_eda_train_dataset\n",
    "\n",
    "# train_dataset = aug_by_eda(train_dataset)\n",
    "# aug_train_dataset = aug_by_eda(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d370cae-5903-460c-a79b-2f6622464d9d",
   "metadata": {},
   "source": [
    "## Data Augmentation by Masked Language Model (ref: [bert_base_uncased](https://huggingface.co/bert-base-uncased?text=what+the+%5BMASK%5D+good))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd5d0f-16cf-4c2f-8776-199256f44c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "if args.masked_lm:\n",
    "    masked_lm = pipeline('fill-mask', model='bert-base-uncased')\n",
    "    # masked_lm(\"Hello I'm a [MASK] model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0995d753-3a3c-45cc-adb1-ef8735a95b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def aug_by_masked_lm(train_dataset, seed, masked_lm, aug_num=3):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    sentences = [train_data[\"sentence\"] for train_data in train_dataset]\n",
    "    labels = [train_data[\"label\"] for train_data in train_dataset]\n",
    "    sentences_len = len(sentences)\n",
    "    \n",
    "    aug_by_masked_lm_train_dataset = train_dataset\n",
    "    for idx in range(sentences_len):\n",
    "        splited_sentences = sentences[idx].split()\n",
    "        \n",
    "        for i in range(aug_num):\n",
    "            target_idx = np.random.choice(len(splited_sentences))\n",
    "            original_word = splited_sentences[target_idx]\n",
    "            \n",
    "            splited_sentences[target_idx] = \"[MASK]\"\n",
    "            \n",
    "            if labels[idx] == 1:\n",
    "                splited_sentences.append(\"positive\")\n",
    "            else:\n",
    "                splited_sentences.append(\"negative\")\n",
    "            \n",
    "            converted_sentence = \" \".join(splited_sentences)\n",
    "            \n",
    "            converted_word = masked_lm(converted_sentence)[0][\"token_str\"]\n",
    "            splited_sentences[target_idx] = converted_word\n",
    "            \n",
    "            aug_data = {\"sentence\": \" \".join(splited_sentences[:-1]), \"label\": labels[idx]}\n",
    "            aug_by_masked_lm_train_dataset = aug_by_masked_lm_train_dataset.add_item(aug_data)\n",
    "            \n",
    "            splited_sentences[target_idx] = original_word\n",
    "        \n",
    "    return aug_by_masked_lm_train_dataset\n",
    "\n",
    "# train_dataset, test_dataset = load_train_test_dataset(0, 32)\n",
    "# aug_by_masked_lm(train_dataset, seed=0, masked_lm=masked_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9156d2-681e-4270-9295-692bf5488738",
   "metadata": {},
   "source": [
    "## Data Augmentation by AFINN using Unlabeled data (ref: [afinn](https://pypi.org/project/afinn/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964c028c-153e-4a3d-b632-267a209de420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "\n",
    "if args.afinn:\n",
    "    afinn = Afinn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091de944-c918-4673-9c4a-31c519e9c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Generate more data with AFIIN \n",
    "def aug_by_afinn(train_dataset, unlabeled_dataset, afinn, data_num=500):\n",
    "    aug_sentences = []\n",
    "    labels = []\n",
    "    \n",
    "    data_num = min(data_num, len(unlabeled_dataset))\n",
    "    progress_bar = tqdm(range(data_num))\n",
    "    \n",
    "    for unlabeled_data in unlabeled_dataset:\n",
    "        sentence = unlabeled_data[\"sentence\"]\n",
    "        \n",
    "        score = afinn.score(sentence)\n",
    "        if score == 0:\n",
    "            continue\n",
    "            \n",
    "        cnt += 1\n",
    "        progress_bar.update(1)\n",
    "        aug_sentences.append(sentence)\n",
    "        labels.append(1 if score > 0 else 0)\n",
    "        \n",
    "        if cnt == data_num:\n",
    "            break\n",
    "    \n",
    "    aug_by_afinn_train_dataset = train_dataset\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        aug_data = {'sentence': sentences[i], 'label': labels[i]}\n",
    "        aug_by_afinn_train_dataset = aug_by_afinn_train_dataset.add_item(aug_data)\n",
    "        \n",
    "    return aug_by_afinn_train_dataset\n",
    "    \n",
    "# aug_by_afinn(train_dataset, unlabeled_dataset, afinn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f3993c-abb1-455e-be65-9e07951eb29d",
   "metadata": {},
   "source": [
    "## Transform Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de10c61-a2db-4ae1-914d-cbe22aa8d2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Tokenize help method\n",
    "def apply_transform(x):\n",
    "    return tokenizer(x[\"sentence\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "def transform_datasets(train_dataset, test_dataset, seed):\n",
    "    tokenized_train_dataset = train_dataset.map(apply_transform, batched=True)\n",
    "    tokenized_test_dataset = test_dataset.map(apply_transform, batched=True)\n",
    "    \n",
    "    # To fit the model's input\n",
    "    tokenized_train_dataset = tokenized_train_dataset.remove_columns(['sentence', 'idx'])\n",
    "    tokenized_train_dataset = tokenized_train_dataset.rename_column(\"label\", \"labels\")\n",
    "    tokenized_test_dataset = tokenized_test_dataset.remove_columns(['sentence', 'idx'])\n",
    "    tokenized_test_dataset = tokenized_test_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "    # labels, input_ids, token_type_idx, attention_mask\n",
    "    # Convert format to torch\n",
    "    tokenized_train_dataset.set_format(\"torch\")\n",
    "    tokenized_test_dataset.set_format(\"torch\")\n",
    "    \n",
    "    tokenized_train_dataset = tokenized_train_dataset.train_test_split(test_size=0.5, seed=seed)\n",
    "    train_dataloader = DataLoader(tokenized_train_dataset[\"train\"], batch_size=8, shuffle=None)\n",
    "    val_dataloader = DataLoader(tokenized_train_dataset[\"test\"], batch_size=8, shuffle=None)\n",
    "    test_dataloader = DataLoader(tokenized_test_dataset, batch_size=8, shuffle=None)\n",
    "    \n",
    "    return (train_dataloader, val_dataloader, test_dataloader)\n",
    "\n",
    "# train_dataloader, val_dataloader, test_dataloader = transform_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b6dabc-5361-4b99-9609-50db14abd0fc",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0413890-2f19-4713-9319-c25c9f9b74bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, optimizer, lr_scheduler, num_epochs=300):\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    best_val_loss = float('inf') \n",
    "    best_state_dict = {}\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        for train_batch, val_batch in zip(train_dataloader, val_dataloader):\n",
    "            train_batch = {k: v.to(DEVICE) for k, v in train_batch.items()}\n",
    "            val_batch = {k: v.to(DEVICE) for k, v in val_batch.items()}\n",
    "            outputs = model(**train_batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            with torch.no_grad():\n",
    "                cur_val_loss = model(**val_batch).loss.item()\n",
    "                val_loss += cur_val_loss \n",
    "                \n",
    "                if cur_val_loss < best_val_loss:\n",
    "                    best_val_loss = cur_val_loss\n",
    "                    best_state_dict = model.state_dict()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "    model.load_state_dict(best_state_dict)\n",
    "    \n",
    "    return (train_losses, val_losses)\n",
    "        \n",
    "# train_losses, val_losses = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84b858e-1d52-46f9-bd4b-fdb9c8539d47",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2c7cf1-d98a-4d84-8ff2-816206bbf19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def evaluate_model(model, test_dataloader):\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    progress_bar = tqdm(range(len(test_dataloader)))\n",
    "\n",
    "    for test_batch in test_dataloader:\n",
    "        test_batch = {k: v.to(DEVICE) for k, v in test_batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**test_batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=test_batch[\"labels\"])\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "    result = metric.compute()\n",
    "    \n",
    "    return result[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a29735-7757-449a-b32c-4fe19962f916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.optim import AdamW\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "learning_rate = 1e-5\n",
    "\n",
    "GPU_NUM = args.gpu_num\n",
    "DEVICE = torch.device(f\"cuda:{GPU_NUM}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "NUM_TRAIN_DATA = args.num_train_data\n",
    "NUM_SEED = args.num_seed\n",
    "NUM_EPOCHS = args.num_epochs\n",
    "\n",
    "MODEL_FOLDER = \"./sst2_bert/\"\n",
    "if not os.path.exists(MODEL_FOLDER):\n",
    "    os.mkdir(MODEL_FOLDER)\n",
    "\n",
    "accuracy = []\n",
    "acc_accuracy = 0.0\n",
    "\n",
    "for seed in range(NUM_SEED):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    train_dataset, train_rest_dataset, test_dataset = load_train_test_dataset(seed=seed, num_train_data=NUM_TRAIN_DATA)\n",
    "    \n",
    "    if args.backt:\n",
    "        train_dataset = aug_by_backt(train_dataset=train_dataset, en_to_others=en_to_others, others_to_en=others_to_en)\n",
    "        \n",
    "    if args.eda:\n",
    "        train_dataset = aug_by_eda(train_dataset=train_dataset)\n",
    "        \n",
    "    if args.masked_lm:\n",
    "        train_dataset = aug_by_masked_lm(train_dataset=train_dataset, seed=seed, masked_lm=masked_lm)\n",
    "        \n",
    "    if args.afinn:\n",
    "        train_dataset = aug_by_afinn(train_dataset=train_dataset, unlabeled_dataset=train_rest_dataset, afinn=afinn)\n",
    "    \n",
    "    train_dataloader, val_dataloader, test_dataloader = transform_datasets(train_dataset=train_dataset,\n",
    "                                                                           test_dataset=test_dataset, \n",
    "                                                                           seed=seed)\n",
    "    \n",
    "    # model preparation\n",
    "    model_name = MODEL_FOLDER + 'model' + str(seed) + '.pth'\n",
    "    if os.path.exists(model_name):\n",
    "        model = torch.load(model_name)\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
    "        torch.save(model, model_name)\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    # training method\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=NUM_EPOCHS*len(train_dataloader)\n",
    "    )\n",
    "    \n",
    "    train_model(model=model, train_dataloader=train_dataloader, val_dataloader=val_dataloader,\n",
    "                optimizer=optimizer, lr_scheduler=lr_scheduler, num_epochs=NUM_EPOCHS)\n",
    "    \n",
    "    cur_accuracy = evaluate_model(model=model, test_dataloader=test_dataloader)\n",
    "    acc_accuracy += cur_accuracy\n",
    "    accuracy.append(cur_accuracy)\n",
    "    \n",
    "table_content = []    \n",
    "table_content.append(\"O\" if args.backt else \"X\")\n",
    "table_content.append(\"O\" if args.eda else \"X\")\n",
    "table_content.append(\"O\" if args.masked_lm else \"X\")\n",
    "table_content.append(acc_accuracy / NUM_SEED)\n",
    "\n",
    "my_table = PrettyTable([\"Backtranslation\", \"EDA\", \"Masked language model\", \"Average Accuracy\"])\n",
    "my_table.add_row(table_content)\n",
    "print(my_table)\n",
    "\n",
    "for i in range((NUM_SEED + 4) // 5):\n",
    "    my_table = PrettyTable([j for j in range(i * 4, min((i + 1) * 4, NUM_SEED))])\n",
    "    my_table.add_row(accuracy[(i * 4): min((i + 1) * 4, NUM_SEED)])\n",
    "    print(my_table)\n",
    "print(my_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
