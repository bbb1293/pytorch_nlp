{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9024322a-060d-4990-8152-7a558880c370",
   "metadata": {},
   "source": [
    "# SST-2 Binary Text Classification with BERT Model (ref: [Transformers](https://huggingface.co/docs/transformers/training), [EDA](https://github.com/jasonwei20/eda_nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c03497b-45de-45f3-a575-634399a7f7ac",
   "metadata": {},
   "source": [
    "## Common Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0be39e9-cacc-44da-9319-a055d32b1110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "SEED = 0\n",
    "train_data_cnt = 32\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6affcd-ab60-42ec-99a1-e0fec74c51bd",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8e333a-2618-4ec7-a68c-b34bfb79fbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "def load_dataset(seed):\n",
    "    dataset = load_dataset(\"sst2\")\n",
    "\n",
    "    # idx, sentence, label\n",
    "    train_dataset = dataset[\"train\"].shuffle(seed=seed).select(range(train_data_cnt))\n",
    "    test_dataset = dataset[\"validation\"]\n",
    "    \n",
    "    return (train_dataset, test_dataset)\n",
    "\n",
    "# train_dataset, test_dataset = load_dataset(seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2211782-a9bb-4208-a160-e9627cfd6c4d",
   "metadata": {},
   "source": [
    "## Data Augmentation by Backtranslation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c132094-84e9-4324-ae34-fe354f132007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "en_to_others = [pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-fr\"), pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-de\")]\n",
    "others_to_en = [pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\"), pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-de-en\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd79da2-755b-4471-987e-1650d6801edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_by_backt(train_dataset, en_to_others=en_to_others, others_to_en=others_to_en):\n",
    "    \n",
    "    sentences = [train_data[\"sentence\"] for train_data in train_dataset]\n",
    "    labels = [train_data[\"label\"] for train_data in train_dataset]\n",
    "    sentences_len = len(sentences)\n",
    "    \n",
    "    aug_by_backt_train_dataset = train_dataset\n",
    "    for translator_idx in range(len(en_to_others)):\n",
    "        tmp_sentence = [tmp_data['translation_text'] for tmp_data in en_to_others[translator_idx](sentences)]\n",
    "        aug_sentence = [tmp_data['translation_text'] for tmp_data in others_to_en[translator_idx](tmp_sentence)]\n",
    "        \n",
    "        for sen_idx in range(sentences_len):\n",
    "            aug_data = {'sentence': aug_sentence[sen_idx], 'label': labels[sen_idx]}\n",
    "            aug_by_backt_train_dataset = aug_by_backt_train_dataset.add_item(aug_data)\n",
    "            \n",
    "    return aug_by_backt_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71ec2f2-457a-4d60-8c54-a4a4c5d4acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = aug_by_backt(train_dataset, en_to_others, others_to_en)\n",
    "# aug_train_dataset = aug_by_backt(train_dataset, en_to_others, others_to_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87983ce2-af58-455b-af93-3e98dc7b0e35",
   "metadata": {},
   "source": [
    "## Data Augmentation by EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11cc973-08cd-4cd8-8eac-fa9669e12b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy data augmentation techniques for text classification\n",
    "# Jason Wei and Kai Zou\n",
    "\n",
    "from eda import *\n",
    "\n",
    "# For the first time to load wordnet\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Generate more data with EDA\n",
    "def aug_by_eda(train_dataset, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, alpha_rd=0.1, num_aug=9):\n",
    "    sentences = [train_data[\"sentence\"] for train_data in train_dataset]\n",
    "    labels = [train_data[\"label\"] for train_data in train_dataset]\n",
    "    \n",
    "    aug_by_eda_train_dataset = train_dataset\n",
    "    aug_sentences = [eda(sentence) for sentence in sentences]\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        for aug_sentence in aug_sentences[i]:\n",
    "            aug_data = {'sentence': aug_sentence, 'label': labels[i]}\n",
    "            aug_by_eda_train_dataset = aug_by_eda_train_dataset.add_item(aug_data)\n",
    "            \n",
    "    return aug_by_eda_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89803f43-0c7d-4156-891b-26c0a519508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = aug_by_eda(train_dataset)\n",
    "# aug_train_dataset = aug_by_eda(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f3993c-abb1-455e-be65-9e07951eb29d",
   "metadata": {},
   "source": [
    "## Transform Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de10c61-a2db-4ae1-914d-cbe22aa8d2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Tokenize help method\n",
    "def apply_transform(x):\n",
    "    return tokenizer(x[\"sentence\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "def transform_datasets(train_dataset, test_dataset, seed):\n",
    "    tokenized_train_dataset = train_dataset.map(apply_transform, batched=True)\n",
    "    tokenized_test_dataset = test_dataset.map(apply_transform, batched=True)\n",
    "    \n",
    "    # To fit the model's input\n",
    "    tokenized_train_dataset = tokenized_train_dataset.remove_columns(['sentence', 'idx'])\n",
    "    tokenized_train_dataset = tokenized_train_dataset.rename_column(\"label\", \"labels\")\n",
    "    tokenized_test_dataset = tokenized_test_dataset.remove_columns(['sentence', 'idx'])\n",
    "    tokenized_test_dataset = tokenized_test_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "    # labels, input_ids, toekn_type_idx, attention_mask\n",
    "    # Convert format to torch\n",
    "    tokenized_train_dataset.set_format(\"torch\")\n",
    "    tokenized_test_dataset.set_format(\"torch\")\n",
    "    \n",
    "    tokenized_train_dataset = tokenized_train_dataset.train_test_split(test_size=0.5, seed=seed)\n",
    "    train_dataloader = DataLoader(tokenized_train_dataset[\"train\"], batch_size=8, shuffle=None)\n",
    "    val_dataloader = DataLoader(tokenized_train_dataset[\"test\"], batch_size=8, shuffle=None)\n",
    "    test_dataloader = DataLoader(tokenized_test_dataset, batch_size=8, shuffle=None)\n",
    "    \n",
    "    return (train_dataloader, val_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf171361-faf7-4dff-a775-7b5f5e0ef105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader, val_dataloader, test_dataloader = transform_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29cee75-0f2f-45c3-bd00-910c45d4d55c",
   "metadata": {},
   "source": [
    "## Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83c6b1d-9a6c-4375-a40f-9785f8942af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4f566f-fa88-4faa-a0dd-c901a3f583dc",
   "metadata": {},
   "source": [
    "## Training Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320b1f62-a223-4454-bd77-92a4e178b31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "learning_rate = 1e-5\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d8da56-d602-4254-b650-fc14ca22403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b6dabc-5361-4b99-9609-50db14abd0fc",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0413890-2f19-4713-9319-c25c9f9b74bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, optimizer, lr_scheduler, num_epochs=300):\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    best_val_loss = float('inf') \n",
    "    best_state_dict = {}\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        for train_batch, val_batch in zip(train_dataloader, val_dataloader):\n",
    "            train_batch = {k: v.to(DEVICE) for k, v in train_batch.items()}\n",
    "            val_batch = {k: v.to(DEVICE) for k, v in val_batch.items()}\n",
    "            outputs = model(**train_batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            with torch.no_grad():\n",
    "                cur_val_loss = model(**val_batch).loss.item()\n",
    "                val_loss += cur_val_loss \n",
    "                \n",
    "                if cur_val_loss < best_val_loss:\n",
    "                    best_val_loss = cur_val_loss\n",
    "                    best_state_dict = model.state_dict\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "    model.load_state_dict(best_state_dict)\n",
    "    \n",
    "    return (train_losses, val_losses)\n",
    "        \n",
    "# train_losses, val_losses = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866822f1-1795-437c-8dd5-50e64bfba6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_train_val_loss(train_losses=train_losses, val_losses=val_losses):\n",
    "    plt.plot(train_losses, \"r\")\n",
    "    plt.plot(val_losses, \"g\")\n",
    "    plt.savefig(\"loss.png\")\n",
    "\n",
    "# plot_train_val_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84b858e-1d52-46f9-bd4b-fdb9c8539d47",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2c7cf1-d98a-4d84-8ff2-816206bbf19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def evalute_model(model, test_dataloader):\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    progress_bar = tqdm(range(len(test_dataloader)))\n",
    "\n",
    "    for test_batch in test_dataloader:\n",
    "        test_batch = {k: v.to(DEVICE) for k, v in test_batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**test_batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=test_batch[\"labels\"])\n",
    "\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    print(metric.compute())\n",
    "    \n",
    "    return metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a29735-7757-449a-b32c-4fe19962f916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "train_data_cnt = 32\n",
    "\n",
    "accuracy = 0.0\n",
    "\n",
    "for seed in range(10):\n",
    "    train_dataset, test_dataset = load_dataset(seed=seed)\n",
    "    \n",
    "    # train_dataset = aug_by_backt()\n",
    "    # train_dataset = aug_by_eda()\n",
    "    \n",
    "    train_dataloader, val_dataloader, test_dataloader = transform_datasets(train_dataset=train_dataset, test_dataset=test_dataset, seed=seed)\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    learning_rate = 1e-5\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    train_model(model=model, train_dataloader=train_dataloader, val_dataloader=val_dataloader, optimizer=optimizer, lr_scheduler=lr_scheduler)\n",
    "    accuracy += evaluate_model(model=mode, test_dataloader)\n",
    "    \n",
    "print(accuracy / 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
