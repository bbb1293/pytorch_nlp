{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9024322a-060d-4990-8152-7a558880c370",
   "metadata": {},
   "source": [
    "# SST-2 Binary Text Classification with BERT Model (ref: [Transformers](https://huggingface.co/docs/transformers/training), [EDA](https://github.com/jasonwei20/eda_nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c03497b-45de-45f3-a575-634399a7f7ac",
   "metadata": {},
   "source": [
    "## Common Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0be39e9-cacc-44da-9319-a055d32b1110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "SEED = 0\n",
    "train_data_cnt = 32\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6affcd-ab60-42ec-99a1-e0fec74c51bd",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8e333a-2618-4ec7-a68c-b34bfb79fbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "dataset = load_dataset(\"sst2\")\n",
    "\n",
    "# idx, sentence, label\n",
    "train_dataset = dataset[\"train\"].shuffle(seed=SEED).select(range(train_data_cnt))\n",
    "test_dataset = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2211782-a9bb-4208-a160-e9627cfd6c4d",
   "metadata": {},
   "source": [
    "## Data Augmentation by Backtranslation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd79da2-755b-4471-987e-1650d6801edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_by_backt(train_dataset, en_to_others, others_to_en):\n",
    "    \n",
    "    sentences = [train_data[\"sentence\"] for train_data in train_dataset]\n",
    "    labels = [train_data[\"label\"] for train_data in train_dataset]\n",
    "    sentences_len = len(sentences)\n",
    "    \n",
    "    aug_by_backt_train_dataset = train_dataset\n",
    "    for translator_idx in range(len(en_to_others)):\n",
    "        tmp_sentence = [tmp_data['translation_text'] for tmp_data in en_to_others[translator_idx](sentences)]\n",
    "        aug_sentence = [tmp_data['translation_text'] for tmp_data in others_to_en[translator_idx](tmp_sentence)]\n",
    "        \n",
    "        for sen_idx in range(sentences_len):\n",
    "            aug_data = {'sentence': aug_sentence[sen_idx], 'label': labels[sen_idx]}\n",
    "            aug_by_backt_train_dataset = aug_by_backt_train_dataset.add_item(aug_data)\n",
    "            \n",
    "    return aug_by_backt_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df717dcb-01db-4983-baeb-2fd6a0a45b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "en_to_others = [pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-fr\"), pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-de\")]\n",
    "others_to_en = [pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\"), pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-de-en\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71ec2f2-457a-4d60-8c54-a4a4c5d4acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = aug_by_backt(train_dataset, en_to_others, others_to_en)\n",
    "aug_train_dataset = aug_by_backt(train_dataset, en_to_others, others_to_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87983ce2-af58-455b-af93-3e98dc7b0e35",
   "metadata": {},
   "source": [
    "## Data Augmentation by EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11cc973-08cd-4cd8-8eac-fa9669e12b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy data augmentation techniques for text classification\n",
    "# Jason Wei and Kai Zou\n",
    "\n",
    "from eda import *\n",
    "\n",
    "# For the first time to load wordnet\n",
    "'''\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "'''\n",
    "\n",
    "# Generate more data with EDA\n",
    "def aug_by_eda(train_dataset, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, alpha_rd=0.1, num_aug=9):\n",
    "    sentences = [train_data[\"sentence\"] for train_data in train_dataset]\n",
    "    labels = [train_data[\"label\"] for train_data in train_dataset]\n",
    "    \n",
    "    aug_by_eda_train_dataset = train_dataset\n",
    "    aug_sentences = [eda(sentence) for sentence in sentences]\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        for aug_sentence in aug_sentences[i]:\n",
    "            aug_data = {'sentence': aug_sentence, 'label': labels[i]}\n",
    "            aug_by_eda_train_dataset = aug_by_eda_train_dataset.add_item(aug_data)\n",
    "            \n",
    "    return aug_by_eda_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89803f43-0c7d-4156-891b-26c0a519508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = aug_by_eda(train_dataset)\n",
    "aug_train_dataset = aug_by_eda(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f3993c-abb1-455e-be65-9e07951eb29d",
   "metadata": {},
   "source": [
    "## Transform Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de10c61-a2db-4ae1-914d-cbe22aa8d2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Tokenize help method\n",
    "def apply_transform(x):\n",
    "    return tokenizer(x[\"sentence\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(apply_transform, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(apply_transform, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac12096-5773-4837-ba66-df079b7e59b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To fit the model's input\n",
    "tokenized_train_dataset = tokenized_train_dataset.remove_columns(['sentence', 'idx'])\n",
    "tokenized_train_dataset = tokenized_train_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_test_dataset = tokenized_test_dataset.remove_columns(['sentence', 'idx'])\n",
    "tokenized_test_dataset = tokenized_test_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# labels, input_ids, toekn_type_idx, attention_mask\n",
    "# Convert format to torch\n",
    "tokenized_train_dataset.set_format(\"torch\")\n",
    "tokenized_test_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024cda32-88ef-4f09-9ab9-1481bacf33fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = tokenized_train_dataset.train_test_split(test_size=0.5, shuffle=False)\n",
    "train_dataloader = DataLoader(tokenized_train_dataset[\"train\"], batch_size=8, shuffle=None)\n",
    "val_dataloader = DataLoader(tokenized_train_dataset[\"test\"], batch_size=8, shuffle=None)\n",
    "test_dataloader = DataLoader(tokenized_test_dataset, batch_size=8, shuffle=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29cee75-0f2f-45c3-bd00-910c45d4d55c",
   "metadata": {},
   "source": [
    "## Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83c6b1d-9a6c-4375-a40f-9785f8942af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4f566f-fa88-4faa-a0dd-c901a3f583dc",
   "metadata": {},
   "source": [
    "## Training Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320b1f62-a223-4454-bd77-92a4e178b31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "learning_rate = 1e-5\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d8da56-d602-4254-b650-fc14ca22403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b6dabc-5361-4b99-9609-50db14abd0fc",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0413890-2f19-4713-9319-c25c9f9b74bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for train_batch, val_batch in zip(train_dataloader, val_dataloader):\n",
    "        train_batch = {k: v.to(DEVICE) for k, v in train_batch.items()}\n",
    "        outputs = model(**train_batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84b858e-1d52-46f9-bd4b-fdb9c8539d47",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2c7cf1-d98a-4d84-8ff2-816206bbf19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "model.eval()\n",
    "\n",
    "progress_bar = tqdm(range(len(test_dataloader)))\n",
    "\n",
    "for test_batch in test_dataloader:\n",
    "    test_batch = {k: v.to(DEVICE) for k, v in test_batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**val_batch)\n",
    "        \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=test_batch[\"labels\"])\n",
    "                    \n",
    "    progress_bar.update(1)\n",
    "    \n",
    "metric.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
